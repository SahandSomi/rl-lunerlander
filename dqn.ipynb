{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "---\n",
    "This notebook contains an implementation of a DQN Agent that solves OpenAI Gym's LunarLander-v2 environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.Model.main import QNetwork\n",
    "from src.Memory.RelpyBuffer import Experience\n",
    "from src.Agent.dqn import DQN\n",
    "from src.Agent.config import DQN_soft_update\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of Actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of Actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DQN_soft_update()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = QNetwork(env.observation_space.shape[0], env.action_space.n, config.SEED)\n",
    "Memory = Experience(env.action_space.n, config.BUFFER_SIZE, config.BATCH_SIZE, device, config.SEED)\n",
    "dqn_Agent = DQN(net, env.observation_space.shape[0], env.action_space.n , Memory, device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahan\\miniconda3\\envs\\RL_Luner\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -79.55\n",
      "Episode 200\tAverage Score: 20.811\n",
      "Episode 300\tAverage Score: 20.44\n",
      "Episode 400\tAverage Score: 29.31\n",
      "Episode 500\tAverage Score: 35.30\n",
      "Episode 600\tAverage Score: 33.63\n",
      "Episode 700\tAverage Score: 31.22\n",
      "Episode 800\tAverage Score: 59.98\n",
      "Episode 900\tAverage Score: 61.63\n",
      "Episode 1000\tAverage Score: 63.66\n",
      "Episode 1100\tAverage Score: 51.19\n",
      "Episode 1200\tAverage Score: 63.08\n",
      "Episode 1300\tAverage Score: 78.97\n",
      "Episode 1400\tAverage Score: 97.21\n",
      "Episode 1500\tAverage Score: 113.06\n",
      "Episode 1600\tAverage Score: 118.49\n",
      "Episode 1700\tAverage Score: 110.14\n",
      "Episode 1800\tAverage Score: 104.68\n",
      "Episode 1900\tAverage Score: 78.458\n",
      "Episode 2000\tAverage Score: 102.63\n",
      "Episode 2100\tAverage Score: 72.900\n",
      "Episode 2200\tAverage Score: 78.72\n",
      "Episode 2300\tAverage Score: 91.48\n",
      "Episode 2400\tAverage Score: 102.57\n",
      "Episode 2500\tAverage Score: 95.833\n",
      "Episode 2600\tAverage Score: 22.49\n",
      "Episode 2700\tAverage Score: 88.12\n",
      "Episode 2800\tAverage Score: 98.42\n",
      "Episode 2900\tAverage Score: 91.297\n",
      "Episode 3000\tAverage Score: 106.33\n",
      "Episode 3100\tAverage Score: 98.250\n",
      "Episode 3200\tAverage Score: 100.12\n",
      "Episode 3300\tAverage Score: 115.46\n",
      "Episode 3400\tAverage Score: 117.37\n",
      "Episode 3500\tAverage Score: 115.95\n",
      "Episode 3600\tAverage Score: 111.79\n",
      "Episode 3700\tAverage Score: 111.30\n",
      "Episode 3800\tAverage Score: 114.28\n",
      "Episode 3900\tAverage Score: 84.129\n",
      "Episode 4000\tAverage Score: -142.28\n"
     ]
    }
   ],
   "source": [
    "def dqn(agent, n_episodes=4000, max_t=200, eps_start=1.0, eps_end=0.01, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(),\"agent_checkpoint\\checkpoint\" + str(i_episode)+\".pth\")\n",
    "        if np.mean(scores_window)>=140.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'agent_checkpoint\\checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn(agent = dqn_Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint100.pth\n",
      "checkpoint200.pth\n",
      "checkpoint300.pth\n",
      "checkpoint400.pth\n",
      "checkpoint500.pth\n",
      "checkpoint600.pth\n",
      "checkpoint700.pth\n",
      "checkpoint800.pth\n",
      "checkpoint900.pth\n",
      "checkpoint1000.pth\n",
      "checkpoint1100.pth\n",
      "checkpoint1200.pth\n",
      "checkpoint1300.pth\n",
      "checkpoint1400.pth\n",
      "checkpoint1500.pth\n",
      "checkpoint1600.pth\n",
      "checkpoint1700.pth\n",
      "checkpoint1800.pth\n",
      "checkpoint1900.pth\n",
      "checkpoint2000.pth\n",
      "checkpoint2100.pth\n",
      "checkpoint2200.pth\n",
      "checkpoint2300.pth\n",
      "checkpoint2400.pth\n",
      "checkpoint2500.pth\n",
      "checkpoint2600.pth\n",
      "checkpoint2700.pth\n",
      "checkpoint2800.pth\n",
      "checkpoint2900.pth\n",
      "checkpoint3000.pth\n",
      "checkpoint3100.pth\n",
      "checkpoint3200.pth\n",
      "checkpoint3300.pth\n",
      "checkpoint3400.pth\n",
      "checkpoint3500.pth\n",
      "checkpoint3600.pth\n",
      "checkpoint3700.pth\n",
      "checkpoint3800.pth\n",
      "checkpoint3900.pth\n",
      "checkpoint4000.pth\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import glob\n",
    "import os\n",
    " \n",
    "# assign directory\n",
    "directory = 'agent_checkpoint/'\n",
    "# iterate over files in\n",
    "# that directory\n",
    "files =  sorted(os.listdir(directory),key=len)\n",
    "for file in files:\n",
    "    # load the weights from file\n",
    "    agent = dqn_Agent\n",
    "    agent.qnetwork_local.load_state_dict(torch.load(directory+file))\n",
    "    print(file)\n",
    "    for i in range(1):\n",
    "        env = gym.make('LunarLander-v2',render_mode=\"human\")\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        for j in range(200):\n",
    "            action = agent.act(state)\n",
    "            env.render()\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
